{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMzN9o5xr7cFZ52EbzUWlKM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Tarea adicional – Práctica de Aprendizaje Automático\n","\n","**Asignatura:** Aprendizaje Automático  \n","**Curso:** 2024–2025  \n","**Trabajo práctico individual/grupal**  \n","**Alumnas:** [Elena Recio Álvarez - 100495725], [Alejandra Castuera García - 100499172]  \n","**Tarea de elección abierta:** Selección de atributos con `SelectKBest`  \n"],"metadata":{"id":"8p7fSPPPdovS"}},{"cell_type":"markdown","source":["## Justificación\n","\n","Como tarea adicional, hemos decidido aplicar un proceso de selección de atributos utilizando `SelectKBest` con la función `mutual_info_classif`.\n","\n","Este método permite seleccionar las variables más relevantes según la información mutua que tienen con la variable objetivo.  \n","Nuestro objetivo con esta técnica es:\n","\n","- Reducir la cantidad de atributos utilizados\n","- Comprobar si se puede mantener un rendimiento similar con menos variables\n","- Simplificar el modelo y acelerar el entrenamiento\n","\n","Es una técnica especialmente útil cuando se trabaja con muchos atributos o modelos más pesados como las SVM.\n","\n"],"metadata":{"id":"iVI_SX9Ad4-b"}},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n","from sklearn.feature_selection import SelectKBest, mutual_info_classif\n","from sklearn.pipeline import Pipeline\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import StandardScaler, OneHotEncoder\n","from sklearn.impute import SimpleImputer\n","from sklearn.svm import SVC\n","from sklearn.metrics import make_scorer, balanced_accuracy_score\n","\n","# Carga del dataset\n","df = pd.read_csv(\"attrition_availabledata_07.csv.gz\")\n","X = df.drop(columns=[\"Attrition\"])\n","y = df[\"Attrition\"]\n","\n","# División train/test\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=1/3, stratify=y, random_state=1725\n",")\n","\n","# Columnas numéricas y categóricas\n","columnas_num = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n","columnas_cat = X_train.select_dtypes(include=\"object\").columns.tolist()\n","\n","# Preprocesado\n","preprocessor = ColumnTransformer([\n","    (\"num\", Pipeline([\n","        (\"imp\", SimpleImputer(strategy=\"mean\")),\n","        (\"scaler\", StandardScaler())\n","    ]), columnas_num),\n","    (\"cat\", Pipeline([\n","        (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n","        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n","    ]), columnas_cat)\n","])\n","\n","# Pipeline con selección de atributos y modelo\n","pipe_kbest = Pipeline([\n","    (\"preprocessor\", preprocessor),\n","    (\"select\", SelectKBest(score_func=mutual_info_classif, k=20)),\n","    (\"modelo\", SVC(kernel=\"rbf\", C=10, gamma=0.01))\n","])\n","\n","# Evaluación con validación cruzada\n","cv_inner = StratifiedKFold(n_splits=5, shuffle=True, random_state=1725)\n","scorer = make_scorer(balanced_accuracy_score)\n","\n","score_kbest = cross_val_score(pipe_kbest, X_train, y_train, cv=cv_inner, scoring=scorer).mean()\n","print(\"SVM con SelectKBest (20 variables) → Balanced Accuracy:\", round(score_kbest, 4))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SkRzfFPNerH0","executionInfo":{"status":"ok","timestamp":1743378898367,"user_tz":-120,"elapsed":7600,"user":{"displayName":"ALEJANDRA CASTUERA GARCIA","userId":"00476257401851556893"}},"outputId":"0aea089b-9539-448c-f87f-91e938406769"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["SVM con SelectKBest (20 variables) → Balanced Accuracy: 0.5362\n"]}]},{"cell_type":"markdown","source":["## Conclusión\n","\n","La selección de variables con `SelectKBest` y `mutual_info_classif` ha reducido el número de atributos a solo 20, sin que eso afecte negativamente al rendimiento del modelo.\n","\n","La SVM con kernel RBF ha mantenido un nivel de balanced accuracy similar al original, lo que indica que muchas variables del dataset no eran realmente relevantes.\n","\n","Esta técnica ha demostrado ser útil para simplificar el modelo y podría aplicarse fácilmente si el dataset fuera más grande o si se quisiera acelerar el entrenamiento en futuras versiones del modelo.\n"],"metadata":{"id":"RSLNpYaVe9gj"}}]}